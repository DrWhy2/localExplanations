---
title: "Example usage"
author: "Mateusz Staniak"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r ex}
library(DALEX)
library(ceterisParibus)
library(randomForest)
library(localModel)
library(glmnet)

data('HR')
HR$evaluation <- as.factor(as.character(HR$evaluation))
mrf <- randomForest(status ~., data = HR, ntree = 100)

explainer <- explain(mrf, HR, HR$status,
                     predict_function = function(x, y) predict(x, y, type = "prob")[, 1])

observation <- HR[10, ]
size <- 1000

model_lok <- individual_surrogate_model(explainer, observation, 2000, n_points = 100,
                                        response_family = "gaussian")
coef(model_lok$model, s = "lambda.min")
```


Local model issues:
  * execution time (smoothing in particular)
  * factor variables (add merging by factorMerger?)
    - without merging, factors with large number of levels overshadow discretized variables
  * idea: identify regions, where linear approximations looks fine based on ceterisParibus?
  * ceterisParibus for categorical response needs to use scores, but it would be best to predict classes (same problem as in lime package & egalitarian)
  * quantile-based cutpoints ignore the data (while adaptive cutpoints are basically lime)
  * flexibility is small (for example one cutpoint vs two cutpoints -> how to assign labels automatically? What about bigger number of cutpoints?)
  
  
