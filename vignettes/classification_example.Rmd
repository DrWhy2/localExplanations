---
title: "Explaining classification models with localModel package"
author: "Mateusz Staniak"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Explaining classification models with localModel package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r ex}
library(DALEX)
library(randomForest)
library(localModel)

data('HR')
HR$evaluation <- as.factor(as.character(HR$evaluation))
mrf <- randomForest(status ~., data = HR, ntree = 100)

explainer <- explain(mrf, 
                     HR[, -6],
                     HR$status,
                     predict_function = function(x, y)
                       as.data.frame(predict(x, y, type = "prob")))
# DALEX2: explainer nie wymaga predict_function dla RF
# Lepsze rozwiązanie, żeby podać y w par. data:
# https://github.com/ModelOriented/ceterisParibus2/blob/master/R/ceteris_paribus.R
#

new_observation <- HR[10, -6]
size <- 1000

model_lok <- individual_surrogate_model(explainer, new_observation, size, 
                                        n_points = 100,
                                        response_family = "gaussian")
# n_points -> grid_points
# Sprawdzić, czy można ustawić liczbę zmiennych do wyboru w glmnet
# Ew. pozwolić ustawiać więcej parametróW 
# Ew. usunąć zbędne zależności dla stabilności
# Podzielić kod komentarzami logicznzie
# similar można utworzyc przez encoded_obs[rep(1m size), ]
# Eksperymentować ze zmianą więcej niż jednej kolumny za jednym razem (liczba zmian: parametr)
# cutTree zamiast optimalPartition
# lime pokazuje tylko jeden przedział dla danej zmiennej
# zaznaczyć, która z wartości była w danych
# różne modele, różne cechy, różne poziomy odpowiedzi mają różne interpretowalne cechy
plot(model_lok)
```
